<!DOCTYPE html>
<html lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-1CMWXLFBCG"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-1CMWXLFBCG');
    </script>

    <meta charset="UTF-8" />
    <title>ALIGNMENT 1: THE ALIGNMENT PROBLEM NO ONE IS TALKING ABOUT</title>
    <meta name="description" content="Exploring the alignment problem in the context of AGI and societal challenges.">
    <meta name="keywords" content="Alignment Problem, AGI, Non-Zero-Sum, Nick Bostrom, Max Tegmark, David Chalmers, Existential Risk">
    <meta name="robots" content="index, follow">
    <meta name="author" content="NonZeroSumJames">
    <a name="Top"></a>
    <link rel="stylesheet" type="text/css" href="style.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

    <!-- For Facebook and LinkedIn -->
    <meta property="og:image" content="https://nonzerosum.games/Images/Social/alignment1.png">
    <meta property="og:title" content="THE ALIGNMENT PROBLEM NO ONE IS TALKING ABOUT">
    <meta property="og:description" content="~ part one - what is alignment? ~">

    <!-- For Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@NonZeroSumJames">
    <meta name="twitter:image" content="https://nonzerosum.games/Images/Social/alignment1.png">
    <meta name="twitter:title" content="THE ALIGNMENT PROBLEM NO ONE IS TALKING ABOUT">
    <meta name="twitter:description" content="~ part one - what is alignment? ~">
</head>

<body>
    <!-- Linking to Menu.html -->
    <script>
        $(document).ready(function() {
            $("#menu").load("menu.html", function() {
                var pageName = window.location.pathname.split("/").pop();
                var menuLink = $("a[href='" + pageName + "']");
                menuLink.parent().remove();
            });
        });
    </script>
    <div id="menu"></div>

    <!-- Content -->
    <h1 style="margin-bottom: 0;">THE ALIGNMENT PROBLEM NO ONE IS TALKING ABOUT</h1>
    <p style="margin-top: 0.2em;">~ what is alignment? ~</p>
    <p><img src="./Images/Content/Alignment_RabbitRobot.png" alt="A Rabbit and Robot aligning"></p>

    <p>When I began exploring <a href="whatarenonzerosumgames.html">non-zero-sum games</a>, I soon discovered that achieving win-win scenarios in the real world is essentially about one thing - the alignment of interests.</p>

    <p><img src="./Images/Content/Alignment_TheFuture.png" alt="The Future of Alignment"></p>

    <p>If you and I both want the same result, we can work together to achieve that goal more efficiently, and create something that is greater than the sum of its parts. However, if we have different interests or if we are both competing for the same finite resource then we are misaligned, and this can lead to <a href="whatarenonzerosumgames.html#ZeroSumGames">zero-sum</a> outcomes.</p>

    <p><img src="./Images/Content/Alignment_Snap.png" alt="Snap! Alignment issues"></p>

    <p>You may have heard the term "alignment" used in the current discourse around existential risk regarding AI, where a key issue is <a href="https://www.rnz.co.nz/national/programmes/saturday/audio/2018884227/brian-christian-ai-s-ethical-alignment-problem" target="_blank">The Alignment Problem</a> or the problem of <em>Goal Alignment</em> which concerns the potential misalignment of goals between humanity and artificial general intelligence (AGI) - a flexible general purpose intelligence, that may have its own motivations and is able to design its own goals.</p>

    <p><img src="./Images/Content/Alignment_MovingApart.png" alt="AGI and Humanity moving apart"></p>

    <p>The argument considers a world where AGI, having decoupled itself from dependency on humans, overtakes human intelligence and develops goals that are at odds with humanity. The most obvious risk is a competition for resources like land, energy or computational capacity, where AGI might attempt to overthrow humanity taking the zero-sum perspective that it's them or us.</p>

    <p><img src="./Images/Content/Alignment_Decoupled.png" alt="Decoupling of goals in AGI"></p>

    <p>But more interesting is the possibility that without developing its own ultimate goals an AGI may develop <em>instrumental goals</em> that help it achieve a directive given by us. For instance, <a href="https://www.youtube.com/watch?v=MnT1xgZgkpk" target="_blank">Nick Bostrom</a>'s paperclip robot is given the directive to create paperclips, and goes ahead converting all matter into paperclips, and in the process destroys all life on earth.</p>

    <p><img src="./Images/Content/Alignment_Collision.png" alt="Collision course with AGI"></p>

    <p>I spent some time with the ideas of Nick Bostrom, Max Tegmark, David Chalmers and other experts in the field while editing the documentary <a href="https://www.youtube.com/watch?v=OVvCGw1Npfg">We need to talk about AI</a> which explored the existential risk posed by the coming AI revolution. Now, one of the benefits of documentary editing (my day job) is that I get to study people at the top of their field and absorb a mass of information that, while it might not make the final cut, paints a vivid and nuanced picture of an issue. In this case, there was a consensus that a key stumbling block in designing AGI that is aligned with humanity's interests is the fact that we cant agree on what humanity's interests are. One of the experts encapsulated this idea in the statement...</p>
        
    <blockquote>We [humanity] need a plan, and we don't have a plan". - <b>Bryan Johnson</b></blockquote>
        

    <p>But how can we come up with a plan if we can't agree on we want? We can often seem misaligned with each other, sometimes because we have different ideas about what we should be striving for, and at other times because we see our collective goals in zero-sum terms, believing that prosperity for some necessitates poverty for others, and that individual interests must be at odds with collective goals.</p>

    <h3>SO...</h3>

    <p>This is what I see as the key alignment problem facing humanity; the alignment between the individual and the collective - a misalignment that plays out in our adversarial political system with right & left wings vying for dominance rather than confluence. We'll be exploring this particular alignment problem and its consequences in <a href="alignment2.html">Part 2</a>.</p>

    <p><img src="./Images/Content/Alignment_HumanitysPlan.png" alt="Humanity's Plan for Alignment"></p>

    <div class="nav-section">
        <span class="part-number"><b>next:<br>HUMANITY'S<br>ALIGNMENT<br>PROBLEM</b></span>
        <a href="alignment2.html" class="nav-link next"><b>>></b></a>
        <a href="alignment6.html" class="nav-link last"><b>>|</b></a>
    </div>

    <footer>
        <h3>RELATED CONTENT</h3>
        <p>
            <ul>
                <li>This is a multi-part blog, which will cover a lot of ground, <a href="alignment2.html">part 2</a> delves deeper into the misaligment of the individual and the collective.</li>
            </ul>
        </p>
    </footer>
    <canvas></canvas>
    <div class="subscribe">
        <a href="subscribe.html" title="SUBSCRIBE" style="text-decoration: none;">8</a>
    </div>
    <script src="./js/alignment_problem.js"></script>
    <div id="giscus_thread"></div>
    <script src="https://giscus.app/client.js"
            data-repo="jamesstephenbrown/nonzerosumgames"
            data-repo-id="MDEwOlJlcG9zaXRvcnkyNDUwNDE4Nzk="
            data-category-id="DIC_kwDODpsK184CA9jn"
            data-mapping="pathname"
            data-reactions-enabled="1"
            data-emit-metadata="0"
            data-input-position="bottom"
            data-theme="dark_dimmed"
            data-lang="en"
            crossorigin="anonymous"
            async>
    </script>
</body>
</html>
